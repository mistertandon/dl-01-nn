{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Please go through this <a href=\"https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0\">brilliant artical</a> before using this jupyter notebook\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1] Basic Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/img_01_nn.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/towards-artificial-intelligence/nothing-but-numpy-understanding-creating-neural-networks-with-computational-graphs-from-scratch-6299901091b0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0],\n",
       "       [0, 1, 1],\n",
       "       [1, 0, 1],\n",
       "       [1, 1, 1]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([[0, 0, 0], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.6])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "weights: Contains weight whose value neural network would learn.\n",
    "\"\"\"\n",
    "weights = np.array([0.1, 0.6])\n",
    "\n",
    "print(weights.shape)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Initially set bias to Zero\n",
    "\"\"\"\n",
    "bias = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0],\n",
       "       [0, 1],\n",
       "       [1, 0],\n",
       "       [1, 1]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Just extracting input features from data points\n",
    "\"\"\"\n",
    "X_O = data[:, :2]\n",
    "print(X_O.shape)\n",
    "X_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1],\n",
       "       [0, 1, 0, 1]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "As we need to perform dot product in next step, therefore to define valid dimesions\n",
    "we have to take transpose of X_O\n",
    "\"\"\"\n",
    "X = X_O.T\n",
    "print(X.shape)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Extracting output values corresponding to input features\n",
    "\"\"\"\n",
    "Y =  data[:, -1]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_linear_eqn(X, weights, bias):\n",
    "    \"\"\"\n",
    "    FORWARD PROPAGATION\n",
    "    --------------------\n",
    "\n",
    "    Here we are doing linear computations of all examples (On example contains available all input features) in one training\n",
    "    dataset simulataneously.\n",
    "\n",
    "\n",
    "    Where X : array([[0, 0, 0],\n",
    "                     [0, 1, 1],\n",
    "                     [1, 0, 1],\n",
    "                     [1, 1, 1]])\n",
    "\n",
    "    And\n",
    "\n",
    "    weights: array([0.1, 0.6])\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "    Z = np.add(bias, np.dot(weights, X))\n",
    "    \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sigmoid(z):\n",
    "    \"\"\"\n",
    "    We are here trying to squash output to range (0, 1)\n",
    "    Note: Parenthesis implies exclusive boundary values.\n",
    "    \n",
    "    Here we input linear computation to sigmoid function and gets output range (0, 1)\n",
    "    \n",
    "    z: Linear equation.\n",
    "    For this neural network it would be\n",
    "    [\n",
    "        weight[0] * X[0][0] + weight[1] * X[1][0] + b\n",
    "        weight[0] * X[0][1] + weight[1] * X[1][1] + b\n",
    "        weight[0] * X[0][2] + weight[1] * X[1][2] + b\n",
    "        weight[0] * X[0][3] + weight[1] * X[1][3] + b\n",
    "    ]    \n",
    "    \n",
    "    sigmoid output\n",
    "    \"\"\"\n",
    "    \n",
    "    sig = np.divide(1, np.add(1, np.exp(-z)))\n",
    "    \n",
    "    return sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(y, y_hat, m, const = 2):\n",
    "    \"\"\"\n",
    "    As we're considering all example simulataneously therefore we're using\n",
    "    cost function instead loss function.\n",
    "    \n",
    "    We have to sum up all the loos due to each example and compute average.\n",
    "    \"\"\"\n",
    "    \n",
    "    total_cost = np.sum(np.divide(np.subtract(y, y_hat) ** 2, const))\n",
    "    \n",
    "    avg_cost = np.divide(total_cost, m)\n",
    "\n",
    "    return avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute__del_cost__by__del_y_hat(y, y_hat, m):\n",
    "    \"\"\"\n",
    "    here we'are computing gradient of cost function w.r.t y_hat\n",
    "    as we know cost function is (y - y_hat)^2 / ( 2 * m )\n",
    "    Therefore it's gardien would be\n",
    "    \n",
    "    (-1 / m)(y - y_hat)\n",
    "    \n",
    "    avg_grad: vector of m length. Each element contains loss corresponding to each example.\n",
    "    \"\"\"\n",
    "    \n",
    "    grad = -np.subtract(y, y_hat)\n",
    "    \n",
    "    avg_grad = np.divide(grad, m)\n",
    "    \n",
    "    return avg_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute__del_y_hat__by__del_z(y_hat):\n",
    "    \"\"\"\n",
    "        del_y_hat__by__del_z [$(y_hat)/$(z)] \n",
    "            As we know\n",
    "            y_hat = sigmoid(z), therefore,\n",
    "            $(sigmoid(z)) / $(z) : sigmoid(z)[1 - sigmoid(z)] OR [y_hat * (1 - y_hat)] \n",
    "    \"\"\"\n",
    "\n",
    "    one_sub_sigma = np.subtract(1, y_hat)\n",
    "    \n",
    "    grad = np.multiply(y_hat, one_sub_sigma)\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def final_grad_at__z(local__del_cost__by__del_y_hat, local__del_y_hat__by__del_z):\n",
    "    \"\"\"\n",
    "    Here we're computing final gradient at Z node i.e. multiplication of\n",
    "    local gradient at Z node and incming gradient from cost.\n",
    "    ( $cost/$y_hat * $y_hat/$z )\n",
    "    \n",
    "    local__del_cost__by__del_y_hat : Gradient of cost function i.e. ($cost/$y_hat)\n",
    "    grad_y_hat__by__del_z: Local gradient at node Z ($y_hat/$z)\n",
    "    \n",
    "    grad_at_node__z: final gradient at node z\n",
    "    \"\"\"\n",
    "    grad_at_node__z = np.multiply(local__del_cost__by__del_y_hat, local__del_y_hat__by__del_z)\n",
    "    \n",
    "    return grad_at_node__z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute__del_z__by__del_w():\n",
    "    \"\"\"\n",
    "    How to comput local gradient of z w.r.t w ($z/$w)\n",
    "    And \n",
    "    local gradient of z w.r.t b ($z/$b)\n",
    "\n",
    "    1. w is a vector i.e. np.array([0.1, 0.6])\n",
    "\n",
    "    2. We do have 4 examples in training dataset\n",
    "\n",
    "    array([[0, 0],\n",
    "           [0, 1],\n",
    "           [1, 0],\n",
    "           [1, 1]])\n",
    "\n",
    "    local__del_z__by__del_w matrix would contains\n",
    "\n",
    "     __       __               __                    __               __                    __\n",
    "    |           |             |                        |             |                        |\n",
    "    |   $z1/$w  |             |   $z1/$w1    $z1/$w2   |             |   x[0][0]    x[0][1]   |\n",
    "    |           |             |                        |             |                        |\n",
    "    |   $z2/$w  |             |   $z2/$w1    $z2/$w2   |             |   x[1][0]    x[1][1]   |\n",
    "    |           |    ===>     |                        |    ===>     |                        | \n",
    "    |   $z3/$w  |             |   $z3/$w1    $z3/$w2   |             |   x[2][0]    x[2][1]   |\n",
    "    |           |             |                        |             |                        |\n",
    "    |   $z4/$w  |             |   $z4/$w1    $z4/$w2   |             |   x[3][0]    x[3][1]   |\n",
    "    |__       __|             |__                    __|             |__                    __|\n",
    "\n",
    "    In above matrix: \n",
    "\n",
    "        z1 = weights[0] * X[0][0] + weights[1] * X[0][1] + b\n",
    "        z2 = weights[0] * X[1][0] + weights[1] * X[1][1] + b\n",
    "        z3 = weights[0] * X[2][0] + weights[1] * X[2][1] + b\n",
    "        z4 = weights[0] * X[3][0] + weights[1] * X[3][1] + b\n",
    "\n",
    "        AND \n",
    "\n",
    "        w1, w2 = weights\n",
    "    \"\"\"\n",
    "\n",
    "    return X_O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute__del_z__by__del_bias():\n",
    "    \"\"\"\n",
    "    Note: gradient of z w.r.t bias ($z/$b)\n",
    "    \n",
    "    compute__del_z__by__del_bias OR del_z__by__del_b\n",
    "    \n",
    "    As we know that at Z node linear equation would be (In general)\n",
    "    z = np.mdot(weight, X) + b \n",
    "    \n",
    "    Therefore it's differentiation w.r.t bias would be 1\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    return np.ones(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = list()\n",
    "\n",
    "for i in range(5000):\n",
    "\n",
    "    \"\"\"\n",
    "    FORWARD PROPAGATION\n",
    "    --------------------\n",
    "    \"\"\"\n",
    "\n",
    "    Z = compute_linear_eqn(X, weights, bias)\n",
    "\n",
    "    y_hat = compute_sigmoid(Z)\n",
    "\n",
    "    avg_cost = compute_cost(Y, y_hat, len(y_hat))\n",
    "    cost.append(avg_cost)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    BACKWARD PROPAGATION\n",
    "    --------------------\n",
    "\n",
    "    local__del_cost__by__del_y_hat : ($cost/$y_hat)\n",
    "\n",
    "    local__del_y_hat__by__del_z    : ($y_hat/$z)\n",
    "\n",
    "    final__del_cost__by__del_z     : ($y_hat/$z) * ($cost / $y_hat)\n",
    "\n",
    "    local__del_z__by__del_w        : ($z/$w)\n",
    "\n",
    "    local__del_z__by__del_bias     : ($z/$b)\n",
    "\n",
    "    \"\"\"\n",
    "    local__del_cost__by__del_y_hat = compute__del_cost__by__del_y_hat(Y, y_hat, len(y_hat))\n",
    "\n",
    "    local__del_y_hat__by__del_z = compute__del_y_hat__by__del_z(y_hat)\n",
    "\n",
    "    final__del_cost__by__del_z = final_grad_at__z(local__del_cost__by__del_y_hat, local__del_y_hat__by__del_z)\n",
    "\n",
    "    local__del_z__by__del_w = compute__del_z__by__del_w()\n",
    "\n",
    "    local__del_z__by__del_bias = compute__del_z__by__del_bias()\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    UPDATE WEIGHTS\n",
    "    --------------\n",
    "    \"\"\"\n",
    "\n",
    "    final__del_cost__by__del_w = np.dot(final__del_cost__by__del_z, local__del_z__by__del_w)\n",
    "\n",
    "    \"\"\"\n",
    "    lr: Learning rate\n",
    "    \"\"\"\n",
    "    lr = 0.05\n",
    "\n",
    "    lr__mul__weights = np.multiply(final__del_cost__by__del_w, 1)\n",
    "\n",
    "    weights = np.subtract(weights, lr__mul__weights)\n",
    "\n",
    "    \"\"\"\n",
    "    UPDATE BIAS\n",
    "    -----------\n",
    "\n",
    "    final__del_cost__by__del_bias ($cost/$b)\n",
    "    $cost/$b: Change in cost function w.r.t bias\n",
    "    \"\"\"\n",
    "\n",
    "    final__del_cost__by__del_bias = np.sum(np.multiply( final__del_cost__by__del_z, local__del_z__by__del_bias))\n",
    "\n",
    "    bias = bias - np.multiply(lr, final__del_cost__by__del_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001818418740242948,\n",
       " 0.0018180377868284524,\n",
       " 0.0018176569837997042,\n",
       " 0.0018172763310705894,\n",
       " 0.001816895828555071]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cost[-5:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
